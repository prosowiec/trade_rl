{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64dedabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5ab43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\tdef __init__(self, input_dim, output_dim):\n",
    "\t\tsuper(DQN, self).__init__()\n",
    "\t\tself.out_steps = output_dim\n",
    "\n",
    "\t\tself.lstm = nn.LSTM(input_size=input_dim, hidden_size=8, batch_first=True)\n",
    "\t\tself.dropout = nn.Dropout(p=0.2)\n",
    "\t\tself.fc4 = nn.Linear(8, output_dim)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t#_, (h_n, _) = self.lstm(x)  # h_n: [1, batch, lstm_units]\n",
    "\t\th_n, _ = self.lstm(x) \n",
    "\t\th_n = h_n.squeeze(0)\n",
    "\n",
    "\t\tx = self.fc4(h_n)\n",
    "\t\tx = x.view(-1, self.out_steps, 1)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "\n",
    "# W = (96 + 8)*2 * log(96 + 8) \n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.observarion_space = 96\n",
    "        self.action_space = 3\n",
    "        \n",
    "        self.model = DQN(self.observarion_space, self.action_space).to(self.device)\n",
    "        self.target_model = DQN(self.observarion_space, self.action_space).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        #self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001, weight_decay=1e-2)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "        self.REPLAY_MEMORY_SIZE = 50000\n",
    "        self.MIN_REPLAY_MEMORY_SIZE = 300 \n",
    "        self.replay_memory = deque(maxlen=self.REPLAY_MEMORY_SIZE)\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "        #model setings\n",
    "        self.UPDATE_TARGET_EVERY = 2\n",
    "        self.MINIBATCH_SIZE = 16\n",
    "        self.DISCOUNT = 0.99\n",
    "        \n",
    "        self.AGGREGATE_STATS_EVERY = 10\n",
    "        \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self, terminal_state):\n",
    "        if len(self.replay_memory) < self.MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, self.MINIBATCH_SIZE)\n",
    "\n",
    "        # Rozpakowanie danych\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        states_v = torch.from_numpy(np.array(states)).float().to(self.device)\n",
    "        next_states_v = torch.from_numpy(np.array(next_states)).float().to(self.device)\n",
    "        actions_v = torch.tensor(actions, dtype=torch.int64, device=self.device)\n",
    "        rewards_v = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        dones_v = torch.tensor(dones, dtype=torch.bool, device=self.device)        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_qs = self.target_model(next_states_v).flatten(start_dim=1)\n",
    "            max_future_qs = torch.max(target_qs, dim=1)[0]\n",
    "            new_qs = rewards_v + (~dones_v * self.DISCOUNT * max_future_qs)\n",
    "\n",
    "        current_qs = self.model(states_v).flatten(start_dim=1)\n",
    "        predicted_qs = current_qs.gather(1, actions_v.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = self.loss_fn(predicted_qs, new_qs)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        if self.target_update_counter > self.UPDATE_TARGET_EVERY:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    def get_qs(self, state):\n",
    "        state_v = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            qs = self.model(state_v)\n",
    "        return qs #.cpu().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f54b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model załadowany z aapl_best_agent_vc_dimOPT.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programowanie\\trade_opt\\rl_agent.py:195: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from rl_agent import load_dqn_agent\n",
    "trader = DQNAgent()\n",
    "\n",
    "load_dqn_agent(trader, 'aapl_best_agent_vc_dimOPT.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6745d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>train_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-09-30 09:30:00</td>\n",
       "      <td>229.23</td>\n",
       "      <td>231.54</td>\n",
       "      <td>228.85</td>\n",
       "      <td>230.73</td>\n",
       "      <td>4165945</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-09-30 09:45:00</td>\n",
       "      <td>230.75</td>\n",
       "      <td>231.09</td>\n",
       "      <td>229.91</td>\n",
       "      <td>230.12</td>\n",
       "      <td>1293996</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-09-30 10:00:00</td>\n",
       "      <td>230.12</td>\n",
       "      <td>230.52</td>\n",
       "      <td>229.73</td>\n",
       "      <td>229.82</td>\n",
       "      <td>1040607</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-09-30 10:15:00</td>\n",
       "      <td>229.83</td>\n",
       "      <td>230.19</td>\n",
       "      <td>229.50</td>\n",
       "      <td>230.17</td>\n",
       "      <td>1166165</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-09-30 10:30:00</td>\n",
       "      <td>230.16</td>\n",
       "      <td>230.69</td>\n",
       "      <td>229.93</td>\n",
       "      <td>230.28</td>\n",
       "      <td>917249</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-06-26 14:45:00</td>\n",
       "      <td>201.02</td>\n",
       "      <td>201.15</td>\n",
       "      <td>200.88</td>\n",
       "      <td>201.04</td>\n",
       "      <td>659472</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-06-26 15:00:00</td>\n",
       "      <td>201.03</td>\n",
       "      <td>201.05</td>\n",
       "      <td>200.80</td>\n",
       "      <td>200.84</td>\n",
       "      <td>623028</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-06-26 15:15:00</td>\n",
       "      <td>200.84</td>\n",
       "      <td>201.18</td>\n",
       "      <td>200.75</td>\n",
       "      <td>201.18</td>\n",
       "      <td>846185</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-06-26 15:30:00</td>\n",
       "      <td>201.18</td>\n",
       "      <td>201.56</td>\n",
       "      <td>200.99</td>\n",
       "      <td>201.16</td>\n",
       "      <td>1205372</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2025-06-26 15:45:00</td>\n",
       "      <td>201.16</td>\n",
       "      <td>201.56</td>\n",
       "      <td>200.79</td>\n",
       "      <td>200.94</td>\n",
       "      <td>3439738</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4786 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ticker                 date    open    high     low   close   volume  \\\n",
       "0     AAPL  2024-09-30 09:30:00  229.23  231.54  228.85  230.73  4165945   \n",
       "1     AAPL  2024-09-30 09:45:00  230.75  231.09  229.91  230.12  1293996   \n",
       "2     AAPL  2024-09-30 10:00:00  230.12  230.52  229.73  229.82  1040607   \n",
       "3     AAPL  2024-09-30 10:15:00  229.83  230.19  229.50  230.17  1166165   \n",
       "4     AAPL  2024-09-30 10:30:00  230.16  230.69  229.93  230.28   917249   \n",
       "..     ...                  ...     ...     ...     ...     ...      ...   \n",
       "474   AAPL  2025-06-26 14:45:00  201.02  201.15  200.88  201.04   659472   \n",
       "475   AAPL  2025-06-26 15:00:00  201.03  201.05  200.80  200.84   623028   \n",
       "476   AAPL  2025-06-26 15:15:00  200.84  201.18  200.75  201.18   846185   \n",
       "477   AAPL  2025-06-26 15:30:00  201.18  201.56  200.99  201.16  1205372   \n",
       "478   AAPL  2025-06-26 15:45:00  201.16  201.56  200.79  200.94  3439738   \n",
       "\n",
       "    train_split  \n",
       "0         train  \n",
       "1         train  \n",
       "2         train  \n",
       "3         train  \n",
       "4         train  \n",
       "..          ...  \n",
       "474        test  \n",
       "475        test  \n",
       "476        test  \n",
       "477        test  \n",
       "478        test  \n",
       "\n",
       "[4786 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from source.database import read_stock_data\n",
    "\n",
    "\n",
    "ticker = 'AAPL'\n",
    "train_df, val_df ,rl_df,test_df = read_stock_data(ticker)\n",
    "training_set = pd.concat([train_df, val_df ,rl_df,test_df])\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d82542f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym import spaces\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, close_data: pd.DataFrame, window_size=96, initial_cash=100000.0, transaction_cost=0.001):\n",
    "        \"\"\"\n",
    "        close_data: DataFrame z kolumnami = aktywa (np. ['AAPL', 'GOOG']), indeks = daty\n",
    "        window_size: ile dni wstecz jest obserwowane\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "\n",
    "        self.close_data = close_data\n",
    "        self.assets = 1 #close_data.columns.tolist()\n",
    "        self.n_assets = 1 #len(self.assets)\n",
    "        self.window_size = window_size\n",
    "        self.initial_cash = initial_cash\n",
    "        self.transaction_cost = transaction_cost\n",
    "\n",
    "        # Akcje = nowa alokacja portfela (ciągłe)\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)\n",
    "\n",
    "        # Obserwacja = ostatnie close'y (window_size × n_assets) + obecna alokacja + poprzednia alokacja\n",
    "        obs_len = self.window_size * self.n_assets + 2 * self.n_assets\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(obs_len,), dtype=np.float32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Fragment danych close z okna czasowego\n",
    "        window = self.close_data[self.current_step - self.window_size:self.current_step]\n",
    "        #market_flat = window.values.flatten()\n",
    "        return np.concatenate([window, self.portfolio, self.prev_action])\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.portfolio_value = self.initial_cash\n",
    "        self.portfolio = np.array([1.0 / self.n_assets] * self.n_assets)  # równomierna alokacja\n",
    "        self.prev_action = self.portfolio.copy()\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, 0, 1)\n",
    "        action = action / (np.sum(action) + 1e-8)  # normalizacja\n",
    "\n",
    "        prev_prices = self.close_data[self.current_step - 1]\n",
    "        curr_prices = self.close_data[self.current_step]\n",
    "        returns = curr_prices / prev_prices  # proste zwroty\n",
    "\n",
    "        # Aktualizacja wartości portfela\n",
    "        portfolio_return = np.dot(self.portfolio, returns)\n",
    "        self.portfolio_value *= portfolio_return\n",
    "\n",
    "        # Koszt zmiany alokacji\n",
    "        trans_cost = self.transaction_cost * np.sum(np.abs(action - self.portfolio))\n",
    "\n",
    "        reward = np.log(portfolio_return + 1e-8) - trans_cost\n",
    "\n",
    "        self.prev_action = self.portfolio.copy()\n",
    "        self.portfolio = action\n",
    "        self.current_step += 1\n",
    "\n",
    "        done = self.current_step >= len(self.close_data) - 1\n",
    "        info = {'portfolio_value': self.portfolio_value}\n",
    "\n",
    "        return self._get_obs(), reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f425a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self, close_data: pd.DataFrame, window_size=96, initial_cash=100000.0, \n",
    "                 transaction_cost=0.001, max_allocation=0.5):\n",
    "        \"\"\"\n",
    "        close_data: DataFrame z kolumnami = aktywa (np. ['AAPL', 'GOOG']), indeks = daty\n",
    "        window_size: ile dni wstecz jest obserwowane  \n",
    "        max_allocation: maksymalny % portfela do alokacji w jednej transakcji\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        self.close_data = close_data\n",
    "        self.assets = close_data.columns.tolist()\n",
    "        self.n_assets = len(self.assets)\n",
    "        self.window_size = window_size\n",
    "        self.initial_cash = initial_cash\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.max_allocation = max_allocation\n",
    "        \n",
    "        # Akcje = dla każdego aktywa: [trading_action, allocation]\n",
    "        # trading_action: 0=hold, 1=buy, 2=sell (dyskretne)\n",
    "        # allocation: 0.0-1.0 (ciągłe, będzie przeskalowane do max_allocation)\n",
    "        self.action_space = spaces.Dict({\n",
    "            'trader': spaces.MultiDiscrete([3] * self.n_assets),  # buy/sell/hold dla każdego aktywa\n",
    "            'portfolio_manager': spaces.Box(low=0, high=1, shape=(self.n_assets,), dtype=np.float32)  # allocation percentages\n",
    "        })\n",
    "        \n",
    "        # Obserwacja = ostatnie close'y + obecne pozycje + gotówka + poprzednie akcje\n",
    "        obs_len = self.window_size * self.n_assets + self.n_assets + 1 + self.n_assets + self.n_assets\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_len,), dtype=np.float32)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # Fragment danych close z okna czasowego\n",
    "        window_start = max(0, self.current_step - self.window_size)\n",
    "        window = self.close_data.iloc[window_start:self.current_step]\n",
    "        \n",
    "        # Normalizacja cen (względem pierwszej ceny w oknie)\n",
    "        if len(window) > 0:\n",
    "            normalized_window = window / window.iloc[0]\n",
    "            # Jeśli okno jest krótsze niż window_size, wypełnij zerami\n",
    "            if len(normalized_window) < self.window_size:\n",
    "                padding = np.zeros((self.window_size - len(normalized_window), self.n_assets))\n",
    "                market_data = np.vstack([padding, normalized_window.values])\n",
    "            else:\n",
    "                market_data = normalized_window.values\n",
    "        else:\n",
    "            market_data = np.zeros((self.window_size, self.n_assets))\n",
    "        \n",
    "        market_flat = market_data.flatten()\n",
    "        \n",
    "        # Obecne pozycje (jako ułamek całkowitej wartości portfela)\n",
    "        current_prices = self.close_data.iloc[self.current_step-1] if self.current_step > 0 else self.close_data.iloc[0]\n",
    "        total_value = self.cash + np.sum(self.positions * current_prices)\n",
    "        position_ratios = (self.positions * current_prices) / (total_value + 1e-8)\n",
    "        \n",
    "        # Gotówka jako ułamek całkowitej wartości\n",
    "        cash_ratio = self.cash / (total_value + 1e-8)\n",
    "        \n",
    "        # Poprzednie akcje (znormalizowane)\n",
    "        prev_trader_actions = self.prev_trader_actions.astype(np.float32) / 2.0  # 0-2 -> 0-1\n",
    "        prev_allocations = self.prev_allocations\n",
    "        \n",
    "        return np.concatenate([\n",
    "            market_flat, \n",
    "            position_ratios, \n",
    "            [cash_ratio],\n",
    "            prev_trader_actions,\n",
    "            prev_allocations\n",
    "        ])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.cash = self.initial_cash\n",
    "        self.positions = np.zeros(self.n_assets)  # liczba akcji każdego aktywa\n",
    "        self.portfolio_value_history = []\n",
    "        self.prev_trader_actions = np.zeros(self.n_assets)\n",
    "        self.prev_allocations = np.zeros(self.n_assets)\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.current_step >= len(self.close_data):\n",
    "            return self._get_obs(), 0, True, {}\n",
    "        \n",
    "        # Rozpakuj akcje\n",
    "        trader_actions = action['trader']\n",
    "        allocation_percentages = action['portfolio_manager']\n",
    "        \n",
    "        # Przeskaluj alokacje do max_allocation\n",
    "        allocation_percentages = np.clip(allocation_percentages, 0, 1) * self.max_allocation\n",
    "        \n",
    "        curr_prices = self.close_data.iloc[self.current_step].values\n",
    "        prev_prices = self.close_data.iloc[self.current_step - 1].values\n",
    "        \n",
    "        # Oblicz wartość portfela przed transakcjami\n",
    "        prev_portfolio_value = self.cash + np.sum(self.positions * prev_prices)\n",
    "        \n",
    "        total_transaction_cost = 0\n",
    "        executed_actions = []\n",
    "        \n",
    "        # Wykonaj akcje dla każdego aktywa\n",
    "        for i, (trader_action, allocation) in enumerate(zip(trader_actions, allocation_percentages)):\n",
    "            action_info = {\n",
    "                'asset': self.assets[i],\n",
    "                'trader_action': trader_action,\n",
    "                'requested_allocation': allocation,\n",
    "                'executed': False,\n",
    "                'shares_traded': 0,\n",
    "                'cost': 0\n",
    "            }\n",
    "            \n",
    "            if trader_action == 1:  # BUY\n",
    "                # Kup akcje za allocation % obecnej wartości portfela\n",
    "                current_portfolio_value = self.cash + np.sum(self.positions * curr_prices)\n",
    "                investment_amount = current_portfolio_value * allocation\n",
    "                \n",
    "                if investment_amount <= self.cash and investment_amount > 0:\n",
    "                    shares_to_buy = investment_amount / curr_prices[i]\n",
    "                    transaction_cost = investment_amount * self.transaction_cost\n",
    "                    \n",
    "                    self.positions[i] += shares_to_buy\n",
    "                    self.cash -= (investment_amount + transaction_cost)\n",
    "                    total_transaction_cost += transaction_cost\n",
    "                    \n",
    "                    action_info.update({\n",
    "                        'executed': True,\n",
    "                        'shares_traded': shares_to_buy,\n",
    "                        'cost': transaction_cost,\n",
    "                        'investment_amount': investment_amount\n",
    "                    })\n",
    "            \n",
    "            elif trader_action == 2:  # SELL\n",
    "                # Sprzedaj allocation % obecnych pozycji\n",
    "                shares_to_sell = self.positions[i] * allocation\n",
    "                \n",
    "                if shares_to_sell > 0:\n",
    "                    sale_amount = shares_to_sell * curr_prices[i]\n",
    "                    transaction_cost = sale_amount * self.transaction_cost\n",
    "                    \n",
    "                    self.positions[i] -= shares_to_sell\n",
    "                    self.cash += (sale_amount - transaction_cost)\n",
    "                    total_transaction_cost += transaction_cost\n",
    "                    \n",
    "                    action_info.update({\n",
    "                        'executed': True,\n",
    "                        'shares_traded': -shares_to_sell,  # negative for sell\n",
    "                        'cost': transaction_cost,\n",
    "                        'sale_amount': sale_amount\n",
    "                    })\n",
    "            \n",
    "            # trader_action == 0 (HOLD) - nic nie rób\n",
    "            executed_actions.append(action_info)\n",
    "        \n",
    "        # Oblicz wartość portfela po transakcjach\n",
    "        curr_portfolio_value = self.cash + np.sum(self.positions * curr_prices)\n",
    "        \n",
    "        # Oblicz zwrot\n",
    "        portfolio_return = (curr_portfolio_value - prev_portfolio_value) / prev_portfolio_value\n",
    "        \n",
    "        # Reward = log return - koszty transakcji - penalty za nieudane transakcje\n",
    "        base_reward = np.log(1 + portfolio_return + 1e-8)\n",
    "        cost_penalty = total_transaction_cost / prev_portfolio_value\n",
    "        \n",
    "        # Penalty za nieudane transakcje (gdy trader chciał coś zrobić ale się nie udało)\n",
    "        failed_actions = sum(1 for action in executed_actions \n",
    "                           if action['trader_action'] != 0 and not action['executed'])\n",
    "        failure_penalty = failed_actions * 0.01  # małe penalty\n",
    "        \n",
    "        reward = base_reward - cost_penalty - failure_penalty\n",
    "        \n",
    "        # Zapisz poprzednie akcje\n",
    "        self.prev_trader_actions = trader_actions.copy()\n",
    "        self.prev_allocations = allocation_percentages.copy()\n",
    "        \n",
    "        self.portfolio_value_history.append(curr_portfolio_value)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        done = self.current_step >= len(self.close_data) - 1\n",
    "        \n",
    "        info = {\n",
    "            'portfolio_value': curr_portfolio_value,\n",
    "            'cash': self.cash,\n",
    "            'positions': self.positions.copy(),\n",
    "            'transaction_cost': total_transaction_cost,\n",
    "            'portfolio_return': portfolio_return,\n",
    "            'executed_actions': executed_actions,\n",
    "            'failed_actions': failed_actions\n",
    "        }\n",
    "        \n",
    "        return self._get_obs(), reward, done, info\n",
    "    \n",
    "    def get_portfolio_allocation(self):\n",
    "        \"\"\"Zwraca obecną alokację portfela\"\"\"\n",
    "        if self.current_step > 0:\n",
    "            curr_prices = self.close_data.iloc[self.current_step - 1].values\n",
    "            total_value = self.cash + np.sum(self.positions * curr_prices)\n",
    "            \n",
    "            # Alokacja dla każdego aktywa\n",
    "            asset_values = self.positions * curr_prices\n",
    "            allocation = asset_values / (total_value + 1e-8)\n",
    "            \n",
    "            # Alokacja gotówki\n",
    "            cash_allocation = self.cash / (total_value + 1e-8)\n",
    "            \n",
    "            return {\n",
    "                'assets': dict(zip(self.assets, allocation)),\n",
    "                'cash': cash_allocation,\n",
    "                'total_value': total_value,\n",
    "                'positions': dict(zip(self.assets, self.positions))\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def sample_action(self):\n",
    "        \"\"\"Przykładowa akcja do testowania\"\"\"\n",
    "        return {\n",
    "            'trader': self.action_space['trader'].sample(),\n",
    "            'portfolio_manager': self.action_space['portfolio_manager'].sample()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eab6ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from enviroments import TimeSeriesEnv_simple\n",
    "\n",
    "#data = training_set['close'].values\n",
    "#data = None\n",
    "data = training_set['close'].copy()\n",
    "data[ticker] = training_set['close']\n",
    "data = pd.DataFrame(data[ticker])\n",
    "\n",
    "\n",
    "data_split = int(len(data)  * 0.8)\n",
    "\n",
    "train_data = data[:data_split]\n",
    "valid_data = data[data_split:]\n",
    "\n",
    "WINDOW_SIZE = 96\n",
    "env = PortfolioEnv(train_data, window_size=WINDOW_SIZE)\n",
    "valid_env = PortfolioEnv(valid_data,window_size=WINDOW_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "17b5fbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99735622, 0.996056  , 0.99757292, 0.99804967,\n",
       "       1.00030338, 1.00108352, 1.00173363, 1.00221038, 1.00307719,\n",
       "       1.00342392, 1.00251376, 1.00247042, 1.00108352, 0.99965327,\n",
       "       1.00047675, 0.99982664, 1.00130022, 0.99986998, 0.99770294,\n",
       "       1.00112686, 1.00091015, 0.99943657, 0.9986131 , 0.99934989,\n",
       "       1.0062844 , 0.97863303, 0.97663936, 0.97334547, 0.97165518,\n",
       "       0.97265202, 0.97312877, 0.97083171, 0.97382222, 0.97473237,\n",
       "       0.97269536, 0.97299874, 0.97308542, 0.96966151, 0.97065835,\n",
       "       0.96840463, 0.96714775, 0.96974819, 0.97230529, 0.97369219,\n",
       "       0.97373554, 0.97377888, 0.9743423 , 0.97538248, 0.97672604,\n",
       "       0.97685607, 0.97720279, 0.96593421, 0.9652841 , 0.96983487,\n",
       "       0.97356217, 0.97568587, 0.97642266, 0.97707277, 0.97702943,\n",
       "       0.98058337, 0.97971655, 0.97962987, 0.9801933 , 0.97958653,\n",
       "       0.97884974, 0.98010662, 0.98023664, 0.97906644, 0.98088675,\n",
       "       0.98067005, 0.97962987, 0.97967321, 0.9799766 , 0.98084341,\n",
       "       0.98145018, 0.98145018, 0.97958653, 0.97598925, 0.97741949,\n",
       "       0.97477571, 0.97104841, 0.97022494, 0.96888138, 0.97022494,\n",
       "       0.97117843, 0.97234863, 0.97230529, 0.97286872, 0.97204525,\n",
       "       0.96948815, 0.966671  , 0.96632428, 0.96684436, 0.96818793,\n",
       "       0.96870801, 0.        , 1.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env._get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "837512fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNPortfolio(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNPortfolio, self).__init__()\n",
    "        self.out_steps = output_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=32, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # x: [batch, seq_len, features]\n",
    "        #last_hidden = lstm_out[:, -1, :]  # weź ostatni krok\n",
    "        #x = self.dropout(last_hidden)\n",
    "        x = self.fc(lstm_out)\n",
    "        x = x.view(-1, self.out_steps, 1)\n",
    "        x = torch.softmax(x, dim=1)  # alokacja portfela jako rozkład prawdopodobieństwa\n",
    "        return x\n",
    "\n",
    "\n",
    "class Agent_portfolio:\n",
    "    def __init__(self, input_dim=96 + 1, action_dim=1):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.model = DQNPortfolio(input_dim, action_dim).to(self.device)\n",
    "        self.target_model = DQNPortfolio(input_dim, action_dim).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001, weight_decay=1e-2)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.replay_memory = deque(maxlen=50000)\n",
    "        self.MIN_REPLAY_MEMORY_SIZE = 300\n",
    "        self.UPDATE_TARGET_EVERY = 2\n",
    "        self.MINIBATCH_SIZE = 16\n",
    "        self.DISCOUNT = 0.99\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    def train(self, terminal_state):\n",
    "        if len(self.replay_memory) < self.MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_memory, self.MINIBATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        # Tensor preparation\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)  # [B, seq_len, features]\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)  # indeksy\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_qs = self.target_model(next_states)  # [B, A]\n",
    "            max_future_qs = torch.max(target_qs, dim=1)[0]\n",
    "            target = rewards + (~dones * self.DISCOUNT * max_future_qs)\n",
    "\n",
    "        current_qs = self.model(states)  # [B, A]\n",
    "        \n",
    "        print(\"current_qs shape:\", current_qs.shape)  # powinno być [B, A]\n",
    "        print(\"actions shape:\", actions.shape)        # [B]\n",
    "        print(\"actions:\", actions)                    # wartości muszą być w zakresie 0..A-1\n",
    "        \n",
    "        predicted = current_qs.gather(1, actions.unsqueeze(1)).squeeze(1)  # [B]\n",
    "\n",
    "        loss = self.loss_fn(predicted, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "        if self.target_update_counter >= self.UPDATE_TARGET_EVERY:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)  # [1, seq, features]\n",
    "        with torch.no_grad():\n",
    "            action = self.model(state)\n",
    "        return action.cpu().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfab2ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_steps_portfolio(env, model, device=\"cuda:0\"):\n",
    "    state = env.reset()  # state = (close_prices, trader_decision)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        close_prices, trader_action = state  # unpack input\n",
    "\n",
    "        # Przygotuj dane wejściowe do modelu\n",
    "        seq_len = close_prices.shape[0]\n",
    "\n",
    "        # Rozszerz decyzję tradera na sekwencję\n",
    "        trader_seq = np.tile(trader_action.flatten(), (seq_len, 1))  # shape: [seq_len, num_assets*3]\n",
    "\n",
    "        # Połącz dane\n",
    "        model_input = np.concatenate([close_prices, trader_seq], axis=1)  # [seq_len, features]\n",
    "        model_input = torch.tensor(model_input, dtype=torch.float32, device=device).unsqueeze(0)  # [1, seq_len, features]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            allocation = model(model_input)  # shape: [1, num_assets]\n",
    "            allocation = allocation.squeeze(0).cpu().numpy()  # [num_assets]\n",
    "\n",
    "        state, reward, done = env.step(allocation)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7efb4cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m evaluate_revards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n\u001b[1;32m----> 8\u001b[0m portfolio_manager \u001b[38;5;241m=\u001b[39m \u001b[43mAgent_portfolio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[25], line 25\u001b[0m, in \u001b[0;36mAgent_portfolio.__init__\u001b[1;34m(self, input_dim, action_dim)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim \u001b[38;5;241m=\u001b[39m action_dim\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mDQNPortfolio\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m DQNPortfolio(input_dim, action_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:283\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 283\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "reward_all = []\n",
    "evaluate_revards = []\n",
    "\n",
    "from copy import deepcopy\n",
    "portfolio_manager = Agent_portfolio()\n",
    "epsilon = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794b53a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m MIN_EPSILON \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Iterate over episodes\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m max_agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m max_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     64\u001b[0m evaluate_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[2], line 29\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservarion_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m96\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservarion_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservarion_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:283\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 283\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\proso\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "def add_trader_action(trader, current_state):\n",
    "    current_state_tensor = torch.tensor(current_state, dtype=torch.float32).unsqueeze(0).to(trader.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_values = trader.target_model(current_state_tensor)\n",
    "        action = torch.argmax(q_values).item()\n",
    "\n",
    "    current_state = np.round(current_state_tensor.cpu().numpy().flatten(),3).tolist()\n",
    "    #print(current_state)\n",
    "    current_state.append(action)\n",
    "    return current_state #.append(action)    \n",
    "\n",
    "#EPSILON_DECAY = 0.998\n",
    "EPSILON_DECAY = 0.99\n",
    "\n",
    "def train_episode(episode, epsilon):\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    \n",
    "    current_state = env.reset()\n",
    "\n",
    "    #print(np.array(current_state, dtype='float32'))\n",
    "    current_state = add_trader_action(trader, np.array(current_state, dtype='float32'))\n",
    "    #print(current_state)\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.dirichlet(np.ones(1))\n",
    "        else:\n",
    "            action = portfolio_manager.get_action(current_state)\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "        new_state = add_trader_action(trader, np.array(new_state, dtype='float32'))\n",
    "        #print(len(new_state))\n",
    "        episode_reward += reward\n",
    "        portfolio_manager.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        \n",
    "        #if np.random.random() >= .7:\n",
    "        portfolio_manager.train(done)\n",
    "\n",
    "        current_state = new_state\n",
    "        \n",
    "        step += 1\n",
    " \n",
    "    if not episode % 5:\n",
    "            print(f\"Episode: {episode} Total Reward: {env.total_profit} Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "    return episode_reward\n",
    "\n",
    "#super dla 200, batch64\n",
    "EPISODES = 100\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "# Iterate over episodes\n",
    "max_agent = DQNAgent()\n",
    "max_reward = 0\n",
    "evaluate_every = 1\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "    reward = train_episode(episode,epsilon)\n",
    "    \n",
    "    \n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "\n",
    "    \n",
    "    reward_all.append(reward)\n",
    "    #if episode % evaluate_every:\n",
    "    valid_env.reset()\n",
    "    reward_valid_dataset = evaluate_steps_portfolio(valid_env, agent.target_model)\n",
    "    evaluate_revards.append(reward_valid_dataset)\n",
    "    \n",
    "    if reward_valid_dataset > max_reward and episode > 10:\n",
    "        max_reward = reward_valid_dataset\n",
    "        #print(max_reward)\n",
    "        max_agent = deepcopy(agent)\n",
    "    \n",
    "    #print(reward_valid_dataset)\n",
    "    if max_reward > 0 and episode > 10 and reward_valid_dataset / max_reward <= .7:\n",
    "        agent = deepcopy(max_agent)\n",
    "\n",
    "    \n",
    "#bierz Q z target modelu\n",
    "    \n",
    "\n",
    "#przed opt - 18 min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
